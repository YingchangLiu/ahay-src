\published{Geophysical Prospecting (2022)}

\title{Seismic data augmentation for automatic fault\old{s} picking using deep learning}
\author{Nam Pham and Sergey Fomel}
\address{The University of Texas at Austin,\\
John A. and Katherine G. Jackson School of Geosciences,\\ Bureau of Economic Geology,\\
Austin, Texas, USA}

\lefthead{Pham et al.}
\righthead{Image augmentation using GANs}

\maketitle

\begin{abstract}
	We propose a method to generate seismic images with corresponding fault labels for augmenting training data in automatic fault\old{s} detection. Our method is based on two generative adversarial networks: one\old{ network} for creating fault\old{s} system and the other for generating 2D seismic images with faults as a condition. Our method can capture the characteristics of field seismic data during \old{infernce}\new{inference} \old{in order }to generate samples that have properties of both field seismic data and synthetic training data. \old{It is helpful when we use these newly generated data for training a convolutional neural network for fault\old{s} picking, as the training data will resemble the field test data}\new{We then use the newly generated seismic images with corresponding fault labels to train a convolutional neural network for fault picking}. \old{As we use faults system as a condition when generating seismic data, we have correct labels for the generated data.} We test the proposed approach on a 3D field dataset from \new{the} Gulf of Mexico. We use different areas in the field dataset as an input to generate new training data for corresponding fault\old{s} picking models. The results show that \new{the generated training data from} our method help\old{s}\old{ to} improve the fault\old{s} picking models in the targeted areas.  
\end{abstract}

\section{Introduction}
\old{Picking faults}\new{Fault picking} in seismic images is an important step for seismic interpretation, \new{which serves as the essential prerequisite to the sequential steps such as }reservoir characterization, well placement, and carbon sequestration. Faults \old{can be}\new{are usually} recognized by \old{looking for reflection }discontinuities in seismic \old{data}\new{amplitude sections}. Several seismic attributes have been proposed to detect faults such as semblance, coherence \cite[]{marfurt1998,gersztenkorn1999,chopra2007,kington2015,karimi2015,wu2017,wu2018}, and Sobel filter \cite[]{sobel1968,phillips2017}. \new{However, }\old{These}\new{these} attributes are sensitive to noise and require time and expertise.

Recently, several authors propose to use deep learning to pick faults either \old{as a classification problem}\new{by classifying whether there are faults at the center of the seismic images} \cite[]{huang2017,wuclass2018,zhao2018,wuclass2019} or \old{a segmentation problem}\new{segmenting all the pixels of the seismic images into fault and background labels} \cite[]{mauricio2017,wu2019,pham2021,hu2022}. \new{In either case, }\old{The}\new{the} training data can be synthetic or from field seismic data with manual interpretations. Researchers focus on improving the neural network model \old{in order }to improve the results in the test data. However, training data also play a very important role. If we have poor training data or our training data do not have similar charracteristics as testing data, very powerful neural network model can be meaningless \cite[]{rolnick2018}.

Machine learning community are changing from model-centric to data-centric approach\new{es}, \old{in order }to produce high-quality data, and improve the results of simple models \cite[]{song2020,motamedi2021,bartel2021}. \new{In model-centric methods, we improve the inference results in the test data by enhancing the neural network model such as increasing the number of layers or changing the network architectures.} \new{On the other hand, }\old{In}\new{in} data-centric method\new{s}, we fix a neural network architecture and improve on the training data, to produce better results in the test data. Improving training data can mean increasing the amount of training data, correcting false labels, or \old{make}\new{making} training data more diverse and resemble the testing data. Data augmentation is one technique to create diverse training data. With seismic data, from existing training data, we can perform rotation, flipping, adding random noise, or smoothing. \cite{nalepa2019,hu2019,zhu2020,zhao2021,zhang2021} show that data augmentation can improve the generalization of a neural network. However, some augmentation methods can provide negative impacts on the model. For example, if we perform flipping along \new{the} depth axis in the seismic data, we are changing the depositional orders of different geologic layers. If we perform smoothing, we are changing the frequency spectrum, and the frequency can be mismatched with the testing field dataset. Therefore, for traditional augmentation, we need to know some prior information about the testing field dataset and the nature of data.

Another approach to generate more training samples is to use generative models such as generative adversarial networks (GANs) \cite[]{goodfellow2014,wu2016,arjovsky2017,isola2017}. Several researchers propose to use GANs for image-to-image translation in seismic interpretation \cite[]{liu2020,kaur2021}, processing \cite[]{picetti2018,yuan2020,kaur2020,kaur2021}, and inversion \cite[]{mosser2019,kaur2020,wang2021}. However, GAN was first proposed to generate new samples that match the training data \cite[]{goodfellow2014}. \cite{neff2018,zhao2018,sandfort2019,pandey2020} propose to use GANs for training data augmentation in semantic segmentation. GANs can generate samples with corresponding labels and improve the generalization of a neural network model. Therefore, we propose a workflow to use GANs to augment training data for the fault picking task in 2D seismic data. Our workflow generates both faults system as labels and corresponding seismic data. \new{Moreover, we generate different seismic data depending on the field seismic data that we use to impose the prior information. }We then follow the data-centric approach \old{to fix}\new{by fixing} an encoder-decoder network architecture for faults segmentation \cite[]{olaf2015,wu2019} and only \old{change}\new{changing} the training data \new{generated from different prior field data}. Results of \new{fault picking on }field seismic data show that our workflow \new{can use an area of field data to }generate more training samples\new{, which }can improve the effectiveness of our neural network model\new{ in this specific area}. 

\section{Generative Adversarial Networks (GANs)}
GAN\old{s have}\new{ is composed of} a generator and a discriminator, which play a minimax game together \cite[]{goodfellow2014}. \new{The generator is a convolutional neural network consisting of several convolutional and upsampling layers. The discriminator is a covolutional neural network with convolutional and downsampling layers.} A generator takes feature vectors as an input, which is a latent space. \new{The latent space is a simpler, lower-dimensional, hidden representation of the data. }\old{A latent space}\new{It} can be treated as a prior information\old{ that we want} to impose on the generated samples\old{ from the generator}. The latent space can come from a random distribution or can be learned from the training data. \old{The generator is a convolutional neural network consisting of several convolutional and upsampling layers. A }\new{The }discriminator takes the generated and real samples as inputs and produces scores whether the samples \old{coming}\new{come} from the generated or real distribution. \new{The score is a number from 0 to 1. High scores mean the samples are from the real distribution; otherwise, they are from the generated distribution. }The generator generates samples that resemble the training data. The discriminator differentiates between \new{the} generated and real samples. During training, the generator is improving to generate data that the discriminator finds hard to distinguish from real data. At the same time, the discriminator improves its ability to separate the generated data from the training data.

Three common metrics to measure the difference between the generated and real data distributions are Kullback-Leibler divergence \cite[]{kullback1959}, Jensen-Shannon divergenece \cite[]{dagan1997,endres2003}, and Wasserstein distance \cite[]{kantorovich1960}. Wasserstein distance has two advantages over other metrics. It helps the convergence of the neural network and prevents the vanishing gradient problem, for some kinds of distributions \cite[]{arjovsky2017}. For our workflow, we use two Wasserstein GANs \cite[]{arjovsky2017} for generating fault labels and seismic data.    

Besides the latent space as an input for the generator, we can use an additional input as a condition \cite[]{mirza2014,isola2017}. The condition\new{s} can be velocity models \cite[]{kaur2020} or interpreted geologic features. For the network to generate seismic data, we use the fault labels as the condition \old{in order }to generate seismic data with corresponding labels. We also learn the latent space from the training seismic data by a series of convolutional and downsampling layers. During inference, we can feed the field data into the trained model to extract features and use \old{it}\new{them} to generate samples that have characteristics of this field data.

\section{Method}
\subsection{Training Data}
\inputdir{exp}
We use 2D synthetic seismic data from \cite{wusyn2020}\old{. The synthetic seismic data}\new{, which} is generated by a series of geological and geophysical methods and constraints. \old{Some examples of training data are in and corresponding labels are in. Each training example has a size of (128, 128) samples. We have 1800 training seismic images with labels and 200 images for validation.}\new{In summary, there are 1800 training seismic images (Figure~\ref{fig:trains}) and corresponding labels (Figure~\ref{fig:train}). Size of a training example is (128,128) samples.}   
\plot{trains}{width=1.0\columnwidth}{Training data. \new{All the images are plotted with the same colorbar.}}
%\plot{trainffts}{width=1.0\columnwidth}{Training data FK.}
\plot{train}{width=1.0\columnwidth}{Training labels. \new{The values are 1 for fault pixels and 0 for background pixels.}}

\subsection{Generating Faults}
\inputdir{exp}
We use Wasserstein GAN to generate faults system \cite[]{arjovsky2017}. The generator starts with a random vector with 128 features from a Gaussian distribution. The latent space is transformed into 256 2D feature maps of size (16, 16) by a fully-connected layer. These feature maps are upsampled to the size of the training data by a series of upsampling, convolutional layers with rectified linear unit (ReLU) activation. The last layer is a sigmoid activation layer so that the output is from 0 to 1. The discriminator takes generated \new{and real} fault labels\old{ and real fault labels} as inputs and calculates the scores of how real or fake a given input looks. The discriminator consists of several convolutional layers with stride 2 to downsample and a fully-connected layer to produce the score \new{whether the input comes from the real distribution}.

\old{The discriminator must have an ability to differentiate between real and generated data. Therefore, the }\new{The }score produced by the discriminator is maximized for the real samples and minimized for the generated samples. We multiply the class label by the score and take the average to be the loss function of the discriminator. The class labels are -1 for real and 1 for fake.
\begin{equation}
	L_D = \frac{1}{m}\,\sum_{i=1}^{m} \left[D(G(x^{(i)})) - D(x^{(i)})\right],
\end{equation}	
where $L_D$ is the loss fuction of the discriminator, $D$ is the discriminator, $G$ is the generator, $x^{(i)}$ is sample $i$ in a training batch, and $m$ is the total number of samples in a training batch. This loss function also approximates the Wasserstein distance \cite[]{kr1958,arjovsky2017}. However, a condition of this approximation is that the discriminator must follow the Lipschitz constraint \cite[]{kr1958}. \cite{gulrajani2017} penalizes the discriminator if the gradient norm moves away from its target norm value 1. \new{We follow the zero-centered gradient penalty \cite[]{tung2019} to improve the generalization capability of the discriminator.}  

\old{The generator, on}\new{On} the other hand\old{, must generate samples looking real to fool the discriminator. Therefore}, the generator maximizes the score produced by the discriminator for the generated samples. The loss function of the generator is 
\begin{equation}
        L_G = -\frac{1}{m}\,\sum_{i=1}^{m} D(G(x^{(i)})).
\end{equation}

We train the model with 700 epochs. \old{The loss function is in. We can see that the magnitude of generator and discriminator losses decrease during training and converge. The generated faults after 100 epochs, 300 epochs, and 600 epochs are in.}\new{The magnitudes of generator and discriminator losses decrease during training and converge (Figure~\ref{fig:lossfault}).} The faults are more connected and elongated, and small segments are removed, from epoch 100 to epoch 300 and to epoch 600 \new{Figure~\ref{fig:testtrain}}. 
\plot{lossfault}{width=1.0\columnwidth}{Training losses. \new{Pink curve is the generator loss. Green curve is the discriminator loss.}}
\plot{testtrain}{width=1.0\columnwidth}{Generated faults. Top row: After 100 epochs, Middle row: After 300 epochs, Bottom row: After 600 epochs. \new{The values are 1 for fault pixels and 0 for background pixels.}}

\subsection{Generating Seismic Data}
We also use Wasserstein GAN to generate seismic data. However, we use faults system as a condition. We follow the architecture of the generator in \cite{pandey2020} with two branches. The first branch starts with 2D fault labels and downsamples to a size of (8, 8) by a series of convolutional layers, batch normalization layers, LeakyReLU activation, and max-pooling layers. \new{LeakyReLU modifies the ReLU function by having a small slope for negative values instead of flat slope, to allow for a non-zero gradient. }We then add fully-connected layers to learn the statistical information (mean and standard deviation) to create a normal distribution for generating feature vectors capturing characteristics of faults system. 

The second branch starts with the seismic data\old{. We call it as }\new{, which is }a style. We \old{then add}\new{use} a series of convolutional layers, batch normalization layers, and max-pooling layers to downsample. We then add fully-connected layers to learn the statistical information (mean and standard deviation) to create a normal distribution for generating latent space with 128 features. The latent space is then upsampled by a series of convolutional layers, batch normalization layers, LeakyReLU activation, and upsampling layers. We concatenate the intermediate outputs of the second branch with the corresponding upsampled layers from the first branch in order to capture the characteristics of faults system. The discriminator is similar to the discriminnator of the GAN to generate faults system, but takes both fault labels and seismic data as inputs. Besides the similar loss functions in Wasserstein GAN, we add a mean square error between the generated and true seismic data as an additional loss function for the generator.

During inference, we feed the field seismic data as a style and use either generated faults system from the first GAN or manually-generated fault labels as a condition. By doing this, we can generate seismic data, which \old{has}\new{have} characteristics of field data and \old{corresponds}\new{correspond} to the input fault label\new{s}.     

\section{Augmentation Results}
After training the first GAN for generating faults system, we feed a random latent space from a normal distribution into the trained generator. \old{The generated faults system are in.}The \new{generated} faults system look feasible and are connected \new{(Figure~\ref{fig:test})}. However, there are some small segments that are not connected in the generated faults.
\plot{test}{width=1.0\columnwidth}{Faults system \new{using latent space from a normal distribution}. \new{The values are 1 for fault pixels and 0 for background pixels.}}

\new{We also use a power-law distribution to generate a random latent space \cite[]{nicol1996,hugh2022}. The generated faults system are in Figure~\ref{fig:testp}.}
\new{\plot{testp}{width=1.0\columnwidth}{Faults system using latent space from a power-law distribution. The values are 1 for fault pixels and 0 for background pixels.}}

To generate seismic data, we use the generated faults from the first GAN as a condition and different seismic data as a style. We first use the synthetic data from \cite{wusyn2020} as a style. The generated seismic data are in Figure~\ref{fig:tests}. There are discontinuities when geologic layers meet the faults. We can also see noise pattern similar to synthetic data in Figure~\ref{fig:trains}. The \new{frequency-wavenumber} \old{FK}\new{(FK)} plot of the generated seismic data (Figure~\ref{fig:ytestsplot}) resembles the FK plot of the synthetic seismic data as a style (Figure~\ref{fig:xtrainplot}). \old{The FK plot is concentrated at the small values in the vertical axis}\new{The FK plot mainly has low wavenumbers and low frequencies. It is symmetric around the zero-wavenumber vertical line}. 
\plot{tests}{width=1.0\columnwidth}{Generated seismic images using Wu's data as style. \new{All the images are plotted with the same colorbar.}}
%\plot{testffts}{width=1.0\columnwidth}{FK of generated seismic images using Wu's data as style.}
\multiplot{2}{xtrainplot,ytestsplot}{width=1.0\columnwidth}{Left: Seismic data, Right: FK. (a) Training data, (b) Generated seismic image using Wu's data as style.}

We want to generate more training samples to improve the effectiveness of a neural network for fault\old{s} picking on field data. We want to capture the characteristics of the field data on new training samples. Therefore, we use a field dataset from \new{the} Gulf of Mexico as a style during inference. We perform two experiments with different styles. We use shallow sections (\old{depth}\new{time} smaller than 2.5 \old{km}\new{seconds}) and deeper sections (\old{depth}\new{time} larger than 2.5 \old{km}\new{seconds}) of all crosslines from a 3D seismic volume (Figure~\ref{fig:xblindc,xblindfftc}).    
\multiplot{2}{xblindc,xblindfftc}{width=0.45\columnwidth}{A crossline of field data. (a) Seismic data with two patches of size (128,128), (b) Two small patches corresponding to the shallow depth (cyan border) and deep depth (green border) of the seismic data.}
%\plot{xblindffti}{width=1.0\columnwidth}{FK of an inline of field seismic data.}
%\plot{testsi}{width=1.0\columnwidth}{Generated seismic images using inlines of field data as style.}
%\plot{testfftsi}{width=1.0\columnwidth}{FK of generated seismic images using inlines of field data as style.}
%\multiplot{2}{xblindc,xblindfftc}{width=0.45\columnwidth}{A crossline of field data. (a) Seismic data, (b) FK.}
%\plot{xblindfftc}{width=1.0\columnwidth}{FK of a crossline of field seismic data.}
%\plot{testsc}{width=1.0\columnwidth}{Generated seismic images using crosslines of field data as style.}
%\plot{testfftsc}{width=1.0\columnwidth}{FK of generated seismic images using crosslines of field data as style.}
%\multiplot{2}{xblindcs,xblindfftcs}{width=0.45\columnwidth}{Shallow section of a crossline of field data. (a) Seismic data, (b) FK.}
%\plot{xblindfftcs}{width=1.0\columnwidth}{FK of shallow section of a crossline of field seismic data.}

Some examples of generated seismic data using shallow depths of all crosslines of field data as style are in Figure~\ref{fig:testscs}. The FK plot of generated seismic image (Figure~\ref{fig:ytestscsplot}) \old{changes and spreads to larger values in the vertical axis. Comparing to, the FK plot also spreads more horizontally}\new{mainly has positive wavenumber}. \old{The FK plot of generated seismic image has similar characteristics of the FK of the style image}\new{The style for the generator comes from the shallow section of the field data with high frequencies and wavenumbers}. Therefore, the generated seismic image captures the features from both training data and style image. Moreover, the faults are clear in the shallow sections of field data. Therefore, the faults in the generated image are also imaged clearly.   
\plot{testscs}{width=1.0\columnwidth}{Generated seismic images using shallow section of crosslines of field data as style. \new{All the images are plotted with the same colorbar.}}
\multiplot{2}{stylescsplot,ytestscsplot}{width=1.0\columnwidth}{Left: Seismic data, Right: FK. (a) Shallow depth of field data as style after normalization, (b) Generated seismic image using shallow depth of crosslines of field data as style.}
%\plot{testfftscs}{width=1.0\columnwidth}{FK of generated seismic images using shallow section of crosslines of field data as style.}
%\multiplot{2}{xblindcd,xblindfftcd}{width=0.45\columnwidth}{Deep section of a crossline of field data. (a) Seismic data, (b) FK.}
%\plot{xblindfftcd}{width=1.0\columnwidth}{FK of deep section of a crossline of field seismic data.}

The deep sections of field data are noisy and blurred. The generated seismic images using deep sections of all crosslines of field data as style are also noisy and the faults are blurred (Figure~\ref{fig:testscd}). The FK plot of generated images (Figure~\ref{fig:ytestscdplot}) also \old{does not spread horizontally}\new{has lower frequencies, when compared to Figure~\ref{fig:ytestscsplot} and Figure~\ref{fig:ytestsplot}.} \new{It }is similar to the FK plot of \new{the} style images (Figure~\ref{fig:stylescdplot}).
\plot{testscd}{width=1.0\columnwidth}{Generated seismic images using deep section of crosslines of field data as style. \new{All the images are plotted with the same colorbar.}}
%\plot{testfftscd}{width=1.0\columnwidth}{FK of generated seismic images using deep section of crosslines of field data as style.}
\multiplot{2}{stylescdplot,ytestscdplot}{width=1.0\columnwidth}{Left: Seismic data, Right: FK. (a) Deep depth of field data as style after normalization, (b) Generated seismic image using deep depth of crosslines of field data as style.}

\section{Fault\old{s} picking}
We generate 1800 seismic images with fault labels and combine with original training data to create a new training dataset. We create three new datasets corresponding to different styles used to generate seismic images. We also create a new training dataset combining original training data with its flipped images. We have a total of 3600 images in \old{the}\new{each} new training dataset.

We train a fixed neural network architecture for fault\old{s} picking and only change the training dataset. The neural network is an encoder-decoder convolutional neural network, which is a simplified version of UNet \cite[]{olaf2015,wu2019}, and has skip-connections between decoder layers and corresponding encoder layers. We then apply the trained model on all crosslines and inlines of the field data to pick faults. \new{We then average the fault probabilities produced when applying the trained model on all crosslines and inlines to get probabilities at every sample in the 3D seismic volume.} For each crossline or inline, we plot the results from the models trained with generated dataset using shallow sections, deep sections and data from \cite{wusyn2020} as style. We also plot the result from the model trained on flipped version of the training data. 

\old{Comparing different plots in the best results are from models trained with generated data using shallow sections as style. The faults are picked clearly at the shallow sections and in the crosslines. We also observe that when having more training data by augmentation, the results are better.}\new{For the first inline (Figure~\ref{fig:xblindi,ypredsegafi,ypredsegafscscombi,ypredsegafscdcombi,ypredsegafscombi,ypredsegafaugi}), the faults prediction from the model trained with flipped augmentation (Figure~\ref{fig:ypredsegafaugi}) are noisier than from the model only trained with original data (Figure~\ref{fig:ypredsegafi}). The faults prediction from the model trained with shallow style augmentation (Figure~\ref{fig:ypredsegafscscombi}) are clean and connected in the shallow section (e.g. from crossline 6650 to crossline 6800). However, the results in the deep section are worse than the results from the model trained with the deep style augmentation (Figure~\ref{fig:ypredsegafscdcombi}) (e.g. around crossline 6650 and deeper than 2.5 seconds). Also, the model trained with \cite{wusyn2020} style augmentation does not produce clean and connected faults in both shallow and deep sections, as the generated images in the training data only resemble the synthetic seismic (Figure~\ref{fig:ypredsegafscombi}).}

\new{For the first crossline (Figure~\ref{fig:xblindc-2,ypredsegafc,ypredsegafscscombc,ypredsegafscdcombc,ypredsegafscombc,ypredsegafaugc}), the model trained with flipped augmentation has many false positives (Figure~\ref{fig:ypredsegafaugc}). The faults produced from the model trained with \cite{wusyn2020} style augmentation are more connected in the shallow section (Figure~\ref{fig:ypredsegafscombc}), when compared to the model trained with only original data (Figure~\ref{fig:ypredsegafc}). However, the model trained with shallow style augmentation can produce even more connected faults in the section above 2.5 seconds (Figure~\ref{fig:ypredsegafscscombc}). The model trained with deep style augmentation has cleaner faults prediction around the salt dome.} 

\new{For the second inline (Figure~\ref{fig:xblindi2,ypredsegafi2,ypredsegafscscombi2,ypredsegafscdcombi2,ypredsegafscombi2,ypredsegafaugi2}), we also observe better results from model trained with shallow style augmentation (Figure~\ref{fig:ypredsegafscscombi2}) in both shallow and deep sections when compared to the results from model only trained with the original data (Figure~\ref{fig:ypredsegafi2}). Moreover, the model trained with deep style augmentation produces fewer false positives in the deep sections (around the salt dome in Figure~\ref{fig:ypredsegafscdcombi2}).} 

\new{For the second crossline (Figure~\ref{fig:xblindc3,ypredsegafc3,ypredsegafscscombc3,ypredsegafscdcombc3,ypredsegafscombc3,ypredsegafaugc3}), the faults prediction are connected in the shallow section from the model trained with shallow style augmentation (Figure~\ref{fig:ypredsegafscscombc3}). The results are cleaner in the deep section from the model trained with deep style augmentation (Figure~\ref{fig:ypredsegafscdcombc3}). Overall, when comparing between different results, the model trained with shallow style augmentation produces the best faults segmentation, as the shallow section in the Gulf of Mexico dataset has appropriate resolution and noise to generate new training images that resemble the field data and not very hard to train for a simple neural network. We still can increase the complexity of the network to get better results in the deep section.}

\new{We also plot the results of two time slices from different models (Figure~\ref{fig:xblindt,ypredsegaft,ypredsegafscscombt,ypredsegafscdcombt,ypredsegafscombt,ypredsegafaugt} at 2 seconds and Figure~\ref{fig:xblindt2,ypredsegaft2,ypredsegafscscombt2,ypredsegafscdcombt2,ypredsegafscombt2,ypredsegafaugt2} at 3 seconds). The results from the model trained with shallow style augmentation exhibit the best lateral and vertical continutities of detected faults (Figure~\ref{fig:ypredsegafscscombt} and Figure~\ref{fig:ypredsegafscscombt2}). Moreover, they also has fewer false positives in the shallow section. However, the predicted faults in the deep section from the model trained with deep style augmentation are more connected and cleaner (Figure~\ref{fig:ypredsegafscdcombt2}). The continuities of the generated seismic images and corresponding fault labels also affect the continuities of the predicted faults in the field test data.} 

\multiplot{6}{xblindi,ypredsegafi,ypredsegafscscombi,ypredsegafscdcombi,ypredsegafscombi,ypredsegafaugi}{width=0.4\columnwidth}{An inline of field data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}
%\plot{ypredsegafi}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscscombi}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscdcombi}{width=1.0\columnwidth}{Fault picking in an inline.}

\multiplot{6}{xblindc-2,ypredsegafc,ypredsegafscscombc,ypredsegafscdcombc,ypredsegafscombc,ypredsegafaugc}{width=0.4\columnwidth}{A crossline of field seismic data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}
%\plot{ypredsegafc}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscscombc}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscdcombc}{width=1.0\columnwidth}{Fault picking in a crossline.}

\multiplot{6}{xblindi2,ypredsegafi2,ypredsegafscscombi2,ypredsegafscdcombi2,ypredsegafscombi2,ypredsegafaugi2}{width=0.4\columnwidth}{An inline of field seismic data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}
%\plot{ypredsegafi2}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscscombi2}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscdcombi2}{width=1.0\columnwidth}{Fault picking in an inline.}
%\multiplot{6}{xblindc2,ypredsegafc2,ypredsegafscscombc2,ypredsegafscdcombc2,ypredsegafscombc2,ypredsegafaugc2}{width=0.4\columnwidth}{A crossline of field seismic data. (a) Seismic data, (b) Fault picking with original training data, (c) Fault picking with original + augmented data (shallow style), (d) Fault picking with original + augmented data (deep style). (e) Fault picking with original + augmented data (\cite{wusyn2020} style). (f) Fault picking with original + augmented data (flipping).}
%\plot{ypredsegafc2}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscscombc2}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscdcombc2}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\multiplot{6}{xblindi3,ypredsegafi3,ypredsegafscscombi3,ypredsegafscdcombi3,ypredsegafscombi3,ypredsegafaugi3}{width=0.4\columnwidth}{An inline of field seismic data. (a) Seismic data, (b) Fault picking with original training data, (c) Fault picking with original + augmented data (shallow style), (d) Fault picking with original + augmented data (deep style). (e) Fault picking with original + augmented data (\cite{wusyn2020} style). (f) Fault picking with original + augmented data (flipping).}
%\plot{ypredsegafi3}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscscombi3}{width=1.0\columnwidth}{Fault picking in an inline.}
%\plot{ypredsegafscdcombi3}{width=1.0\columnwidth}{Fault picking in an inline.}

\multiplot{6}{xblindc3,ypredsegafc3,ypredsegafscscombc3,ypredsegafscdcombc3,ypredsegafscombc3,ypredsegafaugc3}{width=0.4\columnwidth}{A crossline of field seismic data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}
%\plot{ypredsegafc3}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscscombc3}{width=1.0\columnwidth}{Fault picking in a crossline.}
%\plot{ypredsegafscdcombc3}{width=1.0\columnwidth}{Fault picking in a crossline.}

\multiplot{6}{xblindt,ypredsegaft,ypredsegafscscombt,ypredsegafscdcombt,ypredsegafscombt,ypredsegafaugt}{width=0.4\columnwidth}{A time slice of field data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}

\multiplot{6}{xblindt2,ypredsegaft2,ypredsegafscscombt2,ypredsegafscdcombt2,ypredsegafscombt2,ypredsegafaugt2}{width=0.4\columnwidth}{A time slice of field data. (a) Seismic data, (b) Fault\old{s} picking with original training data, (c) Fault\old{s} picking with original + augmented data (shallow style), (d) Fault\old{s} picking with original + augmented data (deep style). (e) Fault\old{s} picking with original + augmented data (\cite{wusyn2020} style). (f) Fault\old{s} picking with original + augmented data (flipping).}

\section{Conclusion}
We propose a workflow to generate new seismic images with fault labels from the synthetic training data. We utilize two Wasserstein Generative Adversarial Networks for generating faults system and seismic data. We capture the characteristics of field data to be a latent space for generating seismic data. We also use faults system as a condition for creating seismic images. After generating new seismic images, we train an encoder-decoder convolutional neural network for fault\old{s} picking. We fix the neural network architecture and only change the training data. The test results on field data show that having more training data improves the inference results. Our method can produce good seismic images with labels for improving the fault\old{s} picking from a neural network.  

\section{Acknowledgement}
We thank Xinming Wu for providing the synthetic dataset for training. We also acknowledge Jake Covault and Zoltan Sylvester for providing field dataset for testing. We thank Texas Consortium for Computational Seismology sponsor for supporting this work. \new{The data that support the findings of this study are available from the corresponding author upon reasonable request.}

\newpage
\onecolumn
\bibliographystyle{seg}
\bibliography{SEG,SEP,paper}



