\published{Geophysics, vol 87, issue 3, U93-U108 (2022)}
\title{A variational approach for picking optimal surfaces from semblance-like panels}
\newtheorem{definition}{Definition}[section]
\author{Luke Decker and Sergey Fomel}
\address{
Oden Institute for Computational Engineering and Sciences \\
The University of Texas at Austin \\
201 E 24th St, \\
Austin, TX 78712 \\
decker.luke@utexas.edu
}
\maketitle

\lefthead{Decker \& Fomel}
\righthead{Variational surface picking}
\begin{abstract}

We propose and demonstrate a variational method for determining optimal velocity fields from semblance-like volumes using continuation. The proposed approach finds a minimal-cost surface through a volume, which often corresponds to a velocity field within a semblance scan.  This allows picked velocity fields to incorporate information from gathers that are spatially near the midpoint in question.  The minimization process amounts to solving a nonlinear elliptic partial differential equation, which is accomplished by changing the elliptic problem to a parabolic one and solving it iteratively until it converges to a critical point which minimizes the cost functional. The continuation approach operates by using a variational framework to iteratively minimize the cost of a velocity surface through successively less-smoothed semblance scans.  The method works because a global minimum for the velocity cost functional can only exist when the semblance scan varies smoothly in space and convexly in the parameter being scanned.  Using a discretization of the functional with a limited-memory Broyden-Fletcher-Goldfarb-Shanno ($\ell$-BFGS) algorithm we illustrate how the continuation approach is able to avoid local minima that would typically capture the iterative solution of an optimal velocity field determined without continuation.  Incorporating continuation enables us to find a lower cost final model which is used for seismic processing of a field data set from the Viking Graben.  We then utilize a field data set from the Gulf of Mexico to show how the final velocity model determined by the method employing continuation is largely independent of the starting velocity model, producing something resembling a global minimum.  Finally, we illustrate the versatility of the variational picking approach by demonstrating how it may be used for automatic interpretation of a seismic horizon from the Heidrun Field.
\end{abstract}

\section{Introduction}


The concept of picking normal moveout (NMO) velocity from the dominant semblance trend in velocity spectral display panels was pioneered by \cite{taner-1969}.   These panels measure how well applying a parameterized operation, like the NMO correction, over a parameter sweep transform data in some metric.  In the case of \cite{taner-1969}, this meant NMO gather flatness as measured by semblance. Numerous other geophysical applications exist including dip moveout (DMO) velocity analysis (\citealp{hale-1984,deregowski-1986,yilmaz-2001}); migration velocity analysis (\citealp{fowler-1988,deregowski-1990,fomelvelcon,deckerovc,decker-prob-diff}); image registration with different time shifts \cite[]{hale-2013}; data alignment by local similarity scan and similar applications (\citealp{attr,fomelvelanal,bader-2019}); deconvolution with dynamic frequency wavelets \cite[]{decker-decon}; picking geobodies, faults, and seismic horizons in automatic interpretation (\citealp{wu-2018,wu2018least,yan-2021}); and determining the principal anisotropic axis in image gathers \cite[]{decker-dtw}.

Because manually selecting such trends can be a labor-intensive process, a rich tradition of research has focused on increasing the ability of computers to learn the dominant trends in semblance-like panels with reduced need for human intervention. This has involved overcoming difficulties in the picking process where artificially or anomalously high semblance values do not reflect a high-quality fit.  These artificially high values are often caused by artifacts related to lateral velocity variations \cite[]{vermeer-1980} and care must be taken to avoid outputting an unphysical velocity model.  Early efforts, like those of \cite{sherwood-1972}, involved interpolating a line between the largest semblance values detected, but this required interpreter supervision to avoid artifacts and unphysical velocity trends.  \cite{bazel-1988} used the theory of geometric optics \cite[]{born-1959} to devise an ``optical stack'' method that automatically output velocity information.  \cite{docin-1995} modified the optical stack approach by applying constraints which force the method to avoid unphysical velocity selections.  \cite{alder-1999} expanded on the method of \cite{docin-1995} by using three-dimensional interpolation and locally scaled regression to determine a dense three-dimensional velocity field. This work was further extended by \cite{siliqi-2003} to solve for two parameters simultaneously: velocity and an annellipticity term which can help account for the effects of dipping reflectors or seismic anisotropy. \cite{arnaud-2004} explored how these picking algorithms may be implemented in situations where the phase or amplitude of a seismic reflection changes with offset.  \cite{larner-2007} demonstrated how selective-correlation velocity analysis could be used to improve the resolution of velocity spectra and lead to more accurate picks. Research has progressed on methods for automatic multiparameter scanning \cite[]{tao-2012}, including the efficient application of the three-dimensional version of the Fourier integral operator butterfly algorithm \cite[]{candes-2009} to the problem  by \cite{hu-2015}.

The approach outlined here follows a branch of inquiry originated by \cite{toldi-1989}, who introduced a method for iteratively finding the best path through semblance using a conjugate-gradient solver given a priori knowledge of the likely velocity field and enforcing penalties for large changes in velocity. \cite{symes-1991} proposed applying a variational principle to invert for velocity models from seismic reflection data using differential semblance.  Differential semblance is advantageous because, in the case of noiseless data, it possesses a single minima and exhibits convex behavior near that minimum, but with a reduction in velocity resolution compared to regular semblance (\citealp{symes-1998,symes-1999,mulder-2002,li-2007}).   \cite{harlan-2001} simplified the method of \cite{toldi-1989}, replacing the a priori constraints of that method with a stiffness penalty and explicitly casting the ideal path through the semblance panel as the maximization of the variational integral:

\begin{equation}
\label{eq:harl-int}
\max_{v(\mathbf{x})} \int \alpha \left[v(\mathbf{x}),\mathbf{x} \right] d\mathbf{x},
\end{equation}
where $v(\mathbf{x})$ is a smooth surface defining the velocity and $\alpha$ is semblance.  This maximization was performed using a Gauss-Newton algorithm \cite[]{luenberger-1984}.  Although \cite{harlan-2001} proposed a framework for solving for $v(\mathbf{x})$, a multidimensional surface, no examples were provided.

Inspired by the work of \cite{Deschamps2001FastEO} in virtual endoscopy, \cite{fomelvelanal} continued with the variational approach of \cite{harlan-2001}.  \cite{fomelvelanal} noted that in the one-dimensional case of a velocity path through a single gather, Equation~\ref{eq:harl-int} could be reformulated to appear analogous to a ray-tracing equation solving for the minimal travel time of the following integral,
\begin{equation}
\label{eq:fomel-var}
\min_{v(t)} \int_{t_{o}}^{t_f} \exp \left(- \alpha \left[v(t),t \right] \right)\sqrt{\lambda^2 + \left( \frac{dv}{dt} \right)^2}dt,
\end{equation}
where $\lambda$ is a parameter that modifies the cost of changing position in $t$ relative to changing position in $v$. Using variational methods (\citealp{lanc-1966,greenberg-1978,gelfand-2000}), the optimal $v(t)$ in Equation~\ref{eq:fomel-var} is determined by solving the eikonal equation (\citealp{babich-1972,yilmaz-2001}) for $U(t,v)$,

\begin{equation}
\label{eq:fomel-eik}
\left(\frac{\partial}{\partial v} U(t,v)\right)^2 + \frac{1}{\lambda^2}\left( \frac{\partial}{\partial t} U(t,v)\right)^2 = \exp \left(-2 \alpha \left[v(t),t \right] \right),
\end{equation}

which, given a $v(t_o)$, may be done using a finite-difference algorithm \cite[]{iserles-1996}.  After determining $U(v,t)$, the method finds $v(t)$ by tracking backward along $\nabla U$ from the $v(t_f)$ that minimizes $U(v,t_f)$. 
Oscillations in $v(t)$ are dampened using shaping regularization \cite[]{fomel-2007}.

We propose to expand the approach of  \cite{fomelvelanal} to picking a multidimensional surface by minimizing a functional resembling semblance-weighted total variation regularization.  This formulation enables  direct use of information from spatially adjacent semblance panels to determine a continuous velocity field without explicitly enforcing smoothing.  It improves upon the existing one-dimensional, gather-by-gather approach of \cite{fomelvelanal} by incorporating spatially adjacent information into the picking algorithm.  The proposed approach is equivalent to a nonlinear elliptic partial differential equation, which can be challenging to solve directly.  Instead, we propose finding minimizing surfaces iteratively. Because common iterative  methods seek out the nearest minimizer for the velocity functional, which can be highly multimodal, successful implementation may require an accurate starting model.  Additionally, gradient or steepest descent methods, which can be used for iteratively finding a minima for a functional, may require many iterations to converge.


Continuation, or graduated optimization (\citealp{Blake87visualreconstruction,chapelle-2006,chaudhuri-2011,hossein-2015,pmlr-v48-hazanb16,xue-2016}), is  a method of non-convex optimization. Using this approach, local minima may be avoided by solving a series of successively more challenging, or less convex, approximations to an optimization problem.  The solution of a 
smoother, or \old{less} \new{more} convex, problem is used as the starting model for more rugose one.  Increasing the convexity of a minimization problem is frequently accomplished by convolving the objective function with a Gaussian kernel \cite[]{wu-1996}. Gaussian convolution may be prohibitively expensive, but may be efficiently approximated by triangle smoothing \cite[]{claerbout1993earth}.  

The limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm ($\ell$-BFGS) refers to a class of quasi-Newton schemes for accelerating the convergence of an iterative minimizer without the need for a large amount of computer memory (\citealp{nocedal-1980,liu-1989,li-2001}).  $\ell$-BFGS works by using several gradient computations to build an approximation for the Hessian of a system.  Such memory considerations become essential for large problems, as the Hessian is on the order of the number of samples in the input vector squared.  Using the approximate Hessian enables the algorithm to draw on information about the curvature of cost function level sets, and thus provide a step direction leading more directly to a minimum.

In the following sections, we propose an extension of \cite{fomelvelanal} for picking velocity surfaces from semblance volumes. The problem is then discretized, and a $\ell$-BFGS algorithm is used to accelerate convergence. We then demonstrate how continuation may be applied to the picking algorithm to bypass local minima and make the algorithm behave more like a global optimizer.  Finally, the automatic picking algorithm is applied to an automatic interpretation problem to illustrate its versatility and applicability beyond picking velocity surfaces from semblance scans.

\section{Theory}

As a natural extension of the one-dimensional functional of \cite{fomelvelanal}  (Equation~\ref{eq:fomel-var}) to higher dimensions, consider the functional
\begin{equation}
    \label{eq:stableminfunc}
        \tilde{G}[v] = \int_\Omega e^{-\alpha[v(\mathbf{x}),\mathbf{x}]}\left( \sqrt{\lambda^2+\left|\nabla v\right|^2} + \frac{\epsilon}{2} \left| \nabla v \right|^2 \right)d\Omega.
\end{equation}
In Equation~\ref{eq:stableminfunc}, we assume that $\lambda$ and $\epsilon$ are strictly positive real numbers, and $\Omega \subset \mathbb{R}^n$ is a convex, bounded set with Lipschitz boundary $\partial \Omega = \Gamma$. Possible solutions $v$ exist in the set of real valued functions $V \subset H^1(\Omega)$, the infinite-dimensional vector space of functions whose gradient components are square integrable, such that $\forall v \in V, \, \nabla v \cdot \hat{\mathbf{n}} = 0$ for $\hat{\mathbf{n}} \in \Gamma$, which is equivalent to a zero-flux boundary condition. $\alpha \in C^1(V\times \Omega, [0,\bar{\alpha}])$ with $0<\bar{\alpha}<
\infty$, or $\alpha$ is a continuous function defined for all plausible velocities $v$ in $V$ and at all spatial locations $\mathbf{x}$ in study area $\Omega$, such that its output is between $0$ and some finite positive number $\bar{\alpha}$.  $\alpha[v,\mathbf{x}]$ is assumed to be a Lipschitz continuous function in $v$ such that $\left| \alpha_v \right| \leq L_{\alpha 1}$, or the magnitude of its partial derivative with respect to $v$ is bounded by a constant, $L_{\alpha 1}$. Furthermore, the partial derivative of $\alpha$ in $v$ is also assumed to be Lipschitz continuous with $\left| \alpha_{vv} \right| \leq L_{\alpha 2}$, or the magnitude of its second partial derivative with respect to $v$ is bounded by a constant, $L_{\alpha 2}$.  We use $\left| \mathbf{x} \right|$ to signify $\sqrt{\mathbf{x} \cdot \mathbf{x}}$ for $\mathbf{x} \in \mathbb{R}^n$, which reduces to the absolute value function if $x\in \mathbb{R}$.  We wish to find the $v \in V$ that minimizes this equation and will serve as the picked velocity model.

$\alpha$ is a semblance-like volume over spatial domain $\Omega$.  $\lambda$ and $\epsilon$ are regularization parameters.  Increasing $\lambda$ makes the minimizing functions more closely follow highs in $\alpha$, while decreasing it makes the output minimizing functions smoother.  $\epsilon$ is a small positive term which penalizes changes in minimizing functions and ensures they exists in $H^1(\Omega)$ \cite[]{dobson-1997}.  Although we found that $\epsilon$ may be set to negligibly small values on the order of machine precision in practice, a complete mathematical treatment of this problem, which is beyond the scope of this manuscript but may be found in Chapter 5 of \cite{decker-dissertation} and draws inspiration from \cite{dacorogna2004introduction} and \cite{smyrlis-2004}, shows how the $\epsilon$ term is essential for guaranteeing the existence of and convergence to minimizers of  Equation~\ref{eq:stableminfunc} in an infinite-dimensional setting. Again, larger values of $\lambda$ lead to minimizing surfaces which may vary significantly to follow high values of $\alpha$ more closely. Larger values of $\epsilon$ ensure that minimizing surfaces have less total variation.  The examples in this paper use $1 \leq \lambda \leq 5$ and $\epsilon = 10^{-3}$. 

Minimizing surfaces of Equation~\ref{eq:stableminfunc} occur when its Fr\'{e}chet derivative, defined as
\begin{equation}
\label{eq:funcgradgfull}
\begin{split}
    \nabla \tilde{G}(v) =& -\alpha_v[v(\mathbf{x}),\mathbf{x}]e^{ -\alpha[v(\mathbf{x}),\mathbf{x}]}\left(\sqrt{\lambda^2+\left|\nabla v\right|^2}+\frac{\epsilon}{2}\left|\nabla v\right|^2 \right) \\
    &- \nabla \cdot \left(e^{-\alpha[v(\mathbf{x}),\mathbf{x}]}\left[\frac{1}{\sqrt{\lambda^2+\left|\nabla v\right|^2}}+\epsilon \right]\nabla v \right),
\end{split}
\end{equation}
is equal to zero. In the right-hand side of Equations~\ref{eq:stableminfunc} and \ref{eq:funcgradgfull}, gradient and divergence operations are applied in $\mathbf{x} \in \Omega$, where $\Omega \subset \mathbb{R}^n$ is the domain of velocity models, and are based on derivatives for each spatial dimension of $\mathbf{x}$.  On the left-hand side of Equation~\ref{eq:funcgradgfull}, the gradient operation refers to the functional gradient, which is applied to a functional $\tilde{G}[v]$ which maps $v \in H^1(\Omega)$ to a real number. This functional gradient, $\nabla \tilde{G}[v]$, is itself a function defined on $\Omega$.

Directly determining minimizing surfaces involves solving a nonlinear elliptic partial differential equation, which can be challenging.  Instead, we choose to adopt an iterative approach by introducing a pseudo-time coordinate, $\tau$, and solving the parabolic equation
\begin{equation}
\label{eq:pickingparabolic}
    \frac{\partial v}{\partial \tau} = -\nabla \tilde{G}(v),
\end{equation}
by allowing $v$ to evolve in $\tau$ until it converges to a critical $v^\ast$ where $\left|\left|\nabla \tilde{G}(v^\ast)\right|\right|=0$.  This is a common approach for finding the minima of a functional \cite[]{Mawhin2010OriginAE} and suggests the use of an iterative gradient descent scheme.  This method provides a sequence $\{v_i\}_{i\in\mathbb{N}}$ of subsequent approximations  for $v^\ast$, the critical point of $\tilde{G}(v)$.  The update structure given an initial velocity model function $v_0$ is 
\begin{equation}
    \label{eq:graddec}
    v_{i+1} = v_i + \rho_i h_i ,
\end{equation}
where $v_i$ are updated velocity models and $h_i$ are search directions. Control parameters, also known as step sizes, $\{\rho_i\}_{i\in\mathbb{N}}$ satisfy $a \leq \rho_i \leq b$ for positive real numbers $a,b$ whose values are related to the smoothness of $\alpha$ \cite[]{decker-dissertation}. Experiments in this paper use $a=10^{-8}$ and $b=10^{-2}$. At each step in Equation~\ref{eq:graddec}, $\nabla \tilde{G}(v_i)$ is evaluated.  If $\left|\left|\nabla \tilde{G}(v_i)\right|\right| = 0 $ the method stops because it has arrived at a critical point or minimizer.  Otherwise,  $h_i$ is chosen to satisfy $\left<\nabla \tilde{G}(v_i),h_i\right>_{H^1(\Omega)}<0$, where $\left< \cdot , \cdot \right>_{H^1(\Omega)}$ is the $H^1$ inner product on $\Omega$.  In the method of steepest descent $h_i = -\nabla \tilde{G}(v_i)$.  $v_{i+1}$ is computed and the method proceeds to the next step. Example pseudocode for a picking algorithm using continuation can be found in Appendix~\ref{pickalgo}.



An iterative algorithm for minimizing Equation~\ref{eq:stableminfunc} to pick an optimal surface from a semblance-like volume  is implemented in the \texttt{Madagascar} software library \cite[]{madagascar} using NumPy. Parallelism is achieved using Numba, and NumPy functions are rewritten as simple, array-based, object-free operations and performance tested using Numba on a Texas Advanced Computing Center (TACC) Stampede2 compute node with 48 Skylake cores for code optimization of each component helper function.  The Fr\'{e}chet derivative, or functional gradient $\nabla \tilde{G}(v)$, given by Equation~\ref{eq:funcgradgfull} is computed using finite-difference derivative operations and the Numba functions written for the task.  The finite-difference approach was chosen for numerical efficiency.  Representing the large, semblance-like panels that are used for $\alpha[v(\mathbf{x}),\mathbf{x}]$ as FEniCS interpolated functions for finite element implementation was found to be overly expensive, and finite-difference computations yielded satisfactory results at reduced computational cost.  The calculated gradient is utilized with the $\ell$-BFGS two-loop recursion method outlined by \cite{nocedal-1980} for approximating the inverse Hessian and calculating a search direction.  After experimentation on different data sets, we found that a memory size of the past 3 gradient computations for use in the approximation of the inverse Hessian achieved the most rapid cost convergence results.  The step size is determined using the output search direction with a golden-section search (\citealp{kiefer-1953,mordecai-1966}). The method is allowed to iterate and update the model until either a maximum number of iterations are achieved, any step size in the search direction fails to decrease the cost by more than a set amount, or the $L^2$ magnitude of the calculated gradient drops below a threshold. We also found that the Hessian may change rapidly in this problem, leading some search directions output by the $\ell$-BFGS scheme to increase the cost of surfaces for all step sizes.  In this circumstance, the previous gradient computations are ``forgotten'' and the $\ell$-BFGS algorithm is reset, enabling a fresh approximation of the inverse Hessian. Careful readers will also notice a dimensional analysis ambiguity when calculating best-fit surfaces from typical semblance panels using the Fr\'{e}chet derivative defined in Equation~\ref{eq:funcgradgfull}.  Usually, the first axis in these data is seismic trace recording time, while other axes are spatial with units of distance.  In order to convert derivatives in these different units into similar ones, spatial derivatives are multiplied by the velocity model for the previous iteration so they have units of s$^{-1}$. 

Convergence of this algorithm requires at least local convexity (\citealp{dennis-1977,nocedal-1980,li-2001}), which is achieved by enforcing smoothness on $\alpha[v(\mathbf{x}),\mathbf{x}]$ using triangle smoothing filters \cite[]{claerbout1993earth}.  As a practical point, artificially large values in $\alpha[v(\mathbf{x}),\mathbf{x}]$ not related to high-quality fit for the parameter being tested must be muted to prevent them from capturing the cost minimizing surface.  In this paper, we manually define muting functions.

Convergence can only be guaranteed to a local minimizer.  For a unique minimizer to exist, the integrand of Equation~\ref{eq:stableminfunc} would need to be strictly convex in every $v$, $\nabla v$ combination for all $\mathbf{x} \in \Omega$ \cite[]{dacorogna2004introduction}, which places unrealistic burdens on $\alpha$. In order to overcome the multimodality of this problem, we employ continuation, or graduated optimization.  This involves smoothing and scaling the semblance-like volume several times.  Smoothing makes the semblance volume more convex, and scaling increases the attraction of semblance highs for $v(\mathbf{x})$ surfaces. If sufficiently strong smoothing is applied, the problem may become convex \cite[]{pmlr-v48-hazanb16}.  In this paper, the authors choose a set of minimum smoothing radii that capture the finest scale of changes in the semblance-like volume that appear to be representative of the underlying model.  The maximum smoothing radii are determined by multiplying the minimum radii by a sufficiently large number so that smoothing makes panels of the semblance volume appear strictly convex.  We choose a number of smoothing levels, then proceed incrementally from maximum to minimum smoothing in a linear manner.  The semblance scaling factor for each level is chosen to be proportional to the ratio between smoothing at that level and minimum smoothing. A constant-gradient velocity field is used as the starting model on the most-smoothed semblance volume, and the minimizing velocity surface is iteratively determined using the two-loop recursion $\ell$-BFGS scheme until convergence on that model is achieved.  The minimizing surface from one level of smoothing is then used as the starting model on the next-less-smoothed semblance volume.  The method proceeds until finding the cost minimizing velocity  on the least-smoothed model.  Because the method is allowed to operate on semblance-like volumes with different levels of smoothing, it is able to find the macro-trends for the best-fit surface before determining finer components.  This enables the method to behave more like a global optimizer and enjoy applicability to problems beyond picking velocity surfaces from semblance-like volumes as experiments in subsequent sections illustrate.  

\section{Field Data Examples}
\subsection{Viking Graben}
\inputdir{newviking}
\multiplot{2}{viking-cmps,viking-envelope-f}{width=0.65\columnwidth}{(a) Common midpoint gathers from the Viking Graben; (b) Constant velocity DMO stack power.}

We apply the method to a field data example from the Viking Graben featured in \cite{vikingdata}. Figure~\ref{fig:viking-cmps} contains a set of  common midpoint (CMP) gathers which were preprocessed to have multiples removed for times less than 2 seconds (s) using the parabolic Radon transform. 
We apply the DMO method from \cite{fowler-1988} to generate a series of constant-velocity stacks from which the stack power, or envelope, is computed.  This will serve as $\alpha[v,\mathbf{x}]$.  A lower mute is applied to suppress high envelope values corresponding to multiples below 2 s.   Triangle smoothing and scaling, featuring smoothing radii ranging in strength from $r_v=5,\,r_t=30,\,r_x=15$ to $r_v=50,\,r_t=300,\,r_x=150$ and scaling factors ranging from $1$ to $10$, are applied to the muted volume for use in continuation.  The least-smoothed version is shown in Figure~\ref{fig:viking-envelope-f}.  We generate 10 volumes of increasingly aggressive smoothing and scaling and apply the $\ell$-BFGS variational velocity picking algorithm with continuation using those volumes. The picking algorithm is allowed to proceed for a maximum of 20 iterations at each continuation level.  The initial continuation level is shown in Figure~\ref{fig:viking-velo-gather-0}.  The initial model shown in solid black is a linear $v(t)$ spanning the range of scanned velocities and record times.  The final model of this continuation level is shown in dashed white.  The sixth continuation level is shown in Figure~\ref{fig:viking-velo-gather-5}, the ninth in Figure~\ref{fig:viking-velo-gather-8}, and the tenth and final level in Figure~\ref{fig:viking-velo-gather-9}. In each of these panels, the solid black line represents the starting model for the continuation level, which was the final model of the previous one, and the dashed white line is the final model.  Figure~\ref{fig:viking-velo-gather-9} also features the linear initial model in solid red, which is the same as the starting model in Figure~\ref{fig:viking-velo-gather-0}, and the picking output without continuation in dashed green.  This model is generated applying the $\ell$-BFGS picking approach only on this level of smoothing, again with a maximum of 20 iterations. This non-continuation final model is shown in Figure~\ref{fig:viking-no-continuation-velo}, and the final model with continuation is displayed in Figure~\ref{fig:viking-velo-out-9}.  To further illustrate how the two final models track stack power at different positions within the study area, overlays of the starting model (solid fuchsia), non-continuation final model (dashed black) and continuation final model (dashed white) are plotted on stack power at midpoints of 5 km in Figure~\ref{fig:gather-0}, 12 km in Figure~\ref{fig:gather-5} and 18.25 km in Figure~\ref{fig:gather-8}. The cost functional in Equation~\ref{eq:stableminfunc} is evaluated for each update of both methods using the least-smoothed stack power volume visible in Figure~\ref{fig:viking-envelope-f}. % and plotted in Figure~\ref{fig:viking-costs-combined-legend}.  The blue line plots the costs of the method without continuation and the red line plots the costs with continuation. Were the non-continuation model allowed to continue past 20 iterations, its cost would not noticeabl change on this plot.

Notice that the cost for each model never attains zero.  This is because the cost defined by Equation~\ref{eq:stableminfunc} is related to the regularized, incremental surface area of the velocity model, $\sqrt{\lambda^2+\left|\nabla v\right|^2}$ weighted by $e^{-\alpha(t,x,v)}$ at each model position with an additional total variation term $\frac{\epsilon}{2}\left|\nabla v\right|^2$.  Here, $\alpha(t,x,v)$ is the stack power volume at zero-offset time $t$, midpoint $x$, and model velocity $v$.  Since stack power is positive and bounded above by some $\bar{\alpha}$, $e^{-\bar{\alpha}} \leq e^{-\alpha(t,x,v)}$. The surface area term is always greater than or equal to $\lambda$, and the total variation term is greater than or equal to zero.  Thus, the lowest cost a model could possibly attain is $ \Delta t \, \Delta x \, \lambda \, e^{-\bar{\alpha}}$ where $\Delta t$ is the difference between maximum and minimum zero-offset time for the stack power volume and $\Delta x$ is the difference between the maximum and minimum midpoint.  For this example, that bounding minimal cost is roughly 40, but that would entail a constant velocity model through a stack power volume uniformly containing maximum stack power values which, on examination of Figure~\ref{fig:viking-envelope-f}, is clearly not the case.  In effect, the cost function, and consequently the update at each iteration, seeks the model that balances tracking high stack power values with minimal model variation.

\multiplot{4}{viking-velo-gather-0,viking-velo-gather-5,viking-velo-gather-8,viking-velo-gather-9}{width=0.45\columnwidth}{Illustration of continuation picking for midpoint at 9.994 km.  Solid black line plots starting model for each level, dashed white line shows final model. (a) Initial continuation level with strongest smoothing; (b) Middle continuation level with moderate smoothing; (c) Second to last continuation level with weak smoothing; (d) Final continuation level also plotting the initial (solid red) and final models (dashed green) for picking without continuation.}

\multiplot{2}{viking-no-continuation-velo,viking-velo-out-9}{width=0.65\columnwidth}{ (a) Velocity model determined by variational picking algorithm without continuation; (b) Velocity model determined by variational velocity picking algorithm utilizing continuation.}

\multiplot{3}{gather-0,gather-5,gather-8}{width=0.3\columnwidth}{Illustration of the picking output at: (a) 5 km; (b) 12 km; (c) 18.25 km.  Solid fuchsia line plots the starting model, dashed black line plots the non-continuation final model from Figure~\ref{fig:viking-no-continuation-velo}, dashed white line plots the continuation final model from Figure~\ref{fig:viking-velo-out-9}.  Background plots the stack power with minimal smoothing shown in Figure~\ref{fig:viking-envelope-f}. }

%\plot{viking-costs-combined-legend}{width=0.95\columnwidth}{Velocity model cost, $\tilde{G}(v_i)$,  computed using the least-smoothed stack power volume visible in Figure~\ref{fig:viking-envelope-f}.}

We use the continuation velocity field to DMO stack the data, as shown in Figure~\ref{fig:viking-velo-out-9-slice}. Then we apply Kirchhoff migration to the data, as displayed in Figure~\ref{fig:viking-velo-out-9-slice-img}.
The Dix formula \cite[]{dixvelocity} is used to compute interval velocity for the continuation model.  This is a representation of local velocity through material at each region in the subsurface.  Picked velocities are RMS velocities, corresponding to a mean velocity value over the complete ray path.  The interval velocity is used for conversion from the time to the depth domain  following the manner of \cite{zone-2018}.  The depth stretched interval velocity for the continuation model is plotted in Figure~\ref{fig:viking-velo-out-9-dix-depthmapped-aspect-bar} and the corresponding depth stretched image in Figure~\ref{fig:viking-velo-out-9-slice-img-depthmapped-aspect}.

\multiplot{2}{viking-velo-out-9-slice,viking-velo-out-9-slice-img}{width=0.65\columnwidth}{(a) DMO stack of the data set in Figure~\ref{fig:viking-cmps} using the continuation velocity model from Figure~\ref{fig:viking-velo-out-9}; (b) Kirchhoff time migrated images of the DMO stack in Figure~\ref{fig:viking-velo-out-9-slice} using  the continuation velocity model  in Figure~\ref{fig:viking-velo-out-9}. }

Notice that continuation behaves as a more global optimization scheme, enabling the solution to avoid local minima.  In fact, on the first continuation level shown in Figure~\ref{fig:viking-velo-gather-0}, the cost actually increases when calculated on the least-smoothed stack power volume, as shown in Figure~\ref{fig:viking-costs-combined-legend}.  This enables the continuation velocity model to find the dominant trend  where it deviates from the starting model, and produce a final model with a low associated cost. The model found without continuation is only able to follow stack power highs where the starting model is already near those highs, as can be seen around 2.25 s in Figure~\ref{fig:viking-velo-gather-9}, and in the three positions illustrated in Figures~\ref{fig:gather-0}, \ref{fig:gather-5}, and \ref{fig:gather-8}, where the black non-continuation final model primarily either overlies or is close to the initial model shown in fuchsia. The velocity field computed without continuation  finds a local minimum with higher cost than the one found using continuation, as shown in Figure~\ref{fig:viking-costs-combined-legend}.   

\multiplot{2}{viking-velo-out-9-dix-depthmapped-aspect-bar,viking-velo-out-9-slice-img-depthmapped-aspect}{width=0.95\columnwidth}{(a) Interval velocity corresponding to continuation model in Figure~\ref{fig:viking-velo-out-9} transformed to the depth domain; (b) Continuation model image in Figure~\ref{fig:viking-velo-out-9-slice-img} transformed to the depth domain.}

The velocity field determined using continuation, Figure~\ref{fig:viking-velo-out-9}, appears geologically plausible.  Velocity values vary smoothly even though smoothness was not explicitly imposed.  Furthermore, the velocity values in that model are in agreement with velocities computed from vertical seismic profile (VSP)  travel time curves for two wells along the seismic line featured in \cite{vikingdata}.  Seismic events in the DMO stack, Figure~\ref{fig:viking-velo-out-9-slice}, and seismic image, Figure~\ref{fig:viking-velo-out-9-slice-img}, generated using that model tend to be well-focused.  Additionally, depth domain interval velocity traces for the continuation model displayed in Figure~\ref{fig:viking-velo-out-9-dix-depthmapped-aspect-bar}  match the trend of sonic log p-wave velocities at the two well locations along the line \cite[]{vikingdata}.


\subsection{Gulf of Mexico}
\inputdir{global-gom}
We now use a field data set from the Gulf of Mexico to illustrate how the continuation approach coupled with the iterative algorithm for velocity picking proposed here can act like a global minimizer, significantly reducing the dependence of the final model on the starting model. Figure~\ref{fig:gom-cmps-0} contains a set of CMP gathers.  A NMO velocity scan is performed on the gathers to calculate semblance over a range of plausible NMO velocities and displayed in Figure~\ref{fig:gom-vscan-0}.  Ten levels of increasingly aggressive triangle smoothing and scaling, again featuring smoothing radii ranging in strength from $r_v=5,\,r_t=30,\,r_x=15$ to $r_v=50,\,r_t=300,\,r_x=150$ and scaling factors ranging from $1$ to $10$, are applied to this NMO scan to create an $\alpha[v(\mathbf{x}),\mathbf{x}]$ for each of ten continuation levels. 

\multiplot{2}{gom-cmps-0,gom-vscan-0}{width=0.65\columnwidth}{(a) CMP gathers from a Gulf of Mexico field data set; (b) NMO velocity scan for the CMP gathers in Figure~\ref{fig:gom-cmps-0}.}

\multiplot{9}{gom-plotmod-0,gom-plotmod-1,gom-plotmod-2,gom-plotmod-3,gom-plotmod-4,gom-plotmod-5,gom-plotmod-6,gom-plotmod-7,gom-plotmod-8}{width=0.29\columnwidth}{Random selection of nine constant-gradient velocity models from the 125  used as starting models.}


To illustrate how the continuation approach reduces the dependence of the final model on the starting model, we generate 125 constant-gradient velocity models which are clipped to the range of the velocity scan.  A random selection of nine of these is shown in Figures~\ref{fig:gom-plotmod-0} through \ref{fig:gom-plotmod-8}.  These 125 starting models are used with the continuation picking approach over the ten smoothness levels and their cost convergence, or model cost by iteration according to Equation~\ref{eq:stableminfunc} as measured on the least-smoothed semblance volume in Figure~\ref{fig:gom-vscan-0}, is plotted in Figure~\ref{fig:gom-costs}. If convergence is achieved before the maximum possible number of iterations, the final cost is repeated in these plots through the maximum iteration number. The lowest cost achieved is plotted as a dashed black line in that plot.  For comparison, we also use the collection as starting models for iterative picking on the least smooth $\alpha[v(\mathbf{x}),\mathbf{x}]$ without continuation.  That cost convergence is plotted in Figure~\ref{fig:gom-noncont-costs}.  Again, the lowest cost achieved by the continuation picking approach is plotted as a dashed black line and, if convergence is achieved before the maximum possible number of iterations, the final cost is repeated in these plots through the maximum iteration number. 

\multiplot{2}{gom-costs,gom-noncont-costs}{width=0.65\columnwidth}{Cost convergence $\tilde{G}(v_i)$ for the 125 constant-gradient velocity models calculated on the least-smoothed semblance volume (a) using the continuation approach; (b) without continuation.  The lowest cost achieved using continuation is plotted as a dashed black line.}

\multiplot{4}{gom-best-model-final-plot,gom-worst-model-final-plot,gom-nocont-best-model-final-plot,gom-nocont-worst-model-final-plot}{width=0.45\columnwidth}{Lowest and highest cost final models from velocity picking using the 125 constant-gradient starting models: (a) lowest cost  continuation final model; (b) highest cost  continuation final model; (c) lowest cost  non-continuation final model; (d) highest cost  non-continuation final model.}

The final continuation model with the lowest cost ($\tilde{G}(v)=129.597$) is plotted in Figure~\ref{fig:gom-best-model-final-plot}, and the continuation model with the highest cost ($\tilde{G}(v)=129.636$) is shown in Figure~\ref{fig:gom-worst-model-final-plot}.  The lowest cost ($\tilde{G}(v)=130.278$) non-continuation final model can be seen in Figure~\ref{fig:gom-nocont-best-model-final-plot}, and the highest cost ($\tilde{G}(v)=143.778$) non-continuation final model is displayed in Figure~\ref{fig:gom-nocont-worst-model-final-plot}.  To show how the continuation approach tends to produce similar final models, we compute the $H^1$ difference between the lowest cost continuation final model shown in Figure~\ref{fig:gom-best-model-final-plot} and every update for every continuation model update and plot them in $\log$ scale in Figure~\ref{fig:gom-convergence}. If convergence is achieved before the maximum possible number of iterations, the final $H^1$ misfit is repeated in these plots thru the maximum iteration number.  The $H^1$ norm of a function $u(\mathbf{x}) \in H^1(\Omega)$, $\Omega \subset \mathbb{R}^n$ is defined as $\left|\left| u(\mathbf{x}) \right|\right|_{H^1(\Omega)} = \sqrt{\int_{\Omega}\left( u(\mathbf{x})^2 + \sum_{i=1}^{n}\left({\frac{\partial}{\partial x_i} u(\mathbf{x})}\right)^2 \right)d\Omega}$, and the $H^1$ difference or misfit of two functions $u,w \in H^1(\Omega)$ is $\left| \left| u-w \right|\right|_{H^1(\Omega)}$.   This norm is used because it is the norm of the Hilbert space where minimizing surfaces of Equation~\ref{eq:stableminfunc} exist, and the norm in which convergence to minimizers should occur.    The mean $H^1$ difference between the lowest-cost continuation final model and each of the 125 continuation final models is displayed in Figure~\ref{fig:gom-models-h1mis}.  These visualizations are also generated for $\log$ scale misfit between the lowest-cost continuation final model and each non-continuation update, as shown in Figure~\ref{fig:gom-nocont-convergence}, and the mean $H^1$ misfit between non-continuation final models and the lowest-cost continuation final model in Figure~\ref{fig:gom-nocont-models-h1mis}.   Notice that in the continuation $H^1$ convergence plot shown in Figure~\ref{fig:gom-convergence}, the lowest-cost model can be seen in red descending to an isolated end point around -11 near iteration 145.  This is because, in the next iteration, it achieves the lowest cost final model, and thus the $H^1$ misfit is zero, the log of which is undefined.

\multiplot{4}{gom-convergence,gom-models-h1mis,gom-nocont-convergence,gom-nocont-models-h1mis}{width=0.45\columnwidth}{(a) $\log$ plot of $H^1$ misfit, or $H^1$ difference between the lowest-cost continuation final model shown in Figure~\ref{fig:gom-best-model-final-plot}  and continuation picking model updates for all 125 constant-gradient starting models; (b) mean $H^1$ difference between continuation picking final models and the lowest-cost continuation final model in Figure~\ref{fig:gom-best-model-final-plot};  (c) $\log$ plot of $H^1$ difference between the lowest-cost continuation final model shown in Figure~\ref{fig:gom-best-model-final-plot}  and non-continuation picking model updates for all 125 constant-gradient starting models; (d) mean $H^1$ difference between non-continuation picking final models and the lowest-cost continuation final model in Figure~\ref{fig:gom-best-model-final-plot}.}

The lowest cost continuation velocity model, which is subsequently referred to as the ``best model,'' is used for seismic processing of this data set.  The left panel of Figure~\ref{fig:gom-gather-0} visualizes the best model overlaid on the semblance scan for a CMP gather at 8.5 km.  The right panel illustrates that gather after NMO correction with the best model has been applied.  Figure~\ref{fig:gom-best-model-nmo-stk} contains stacked data following NMO correction using the best model.  Diffraction data are extracted from that stack by using plane-wave destruction filters (\citealp{fomel1,diffr,decker}) to predict reflection signal and remove it from zero-offset data, leaving the set of diffractions and noise, which is displayed in Figure~\ref{fig:gom-best-model-nmo-stk-difr}.  The best model is used for Kirchhoff time migration on the NMO stack data, whose image is shown in Figure~\ref{fig:gom-best-model-image}, and on the diffraction data to create the diffraction image in Figure~\ref{fig:gom-best-model-difr-img}.

\plot{gom-gather-0}{width=.9\columnwidth}{Illustration of the effects of the picking algorithm on a CMP centered at 8.5 km.  Left panel contains a semblance scan for the CMP gather centered here overlaid by lowest cost final continuation velocity model from Figure~\ref{fig:gom-best-model-final-plot} in solid fuchsia. Right panel contains the NMO corrected gather  using that lowest cost final velocity model.}

\multiplot{2}{gom-best-model-nmo-stk,gom-best-model-nmo-stk-difr}{width=0.58\columnwidth}{(a) NMO corrected stack generated using the lowest cost continuation model shown in Figure~\ref{fig:gom-best-model-final-plot}; (b) Diffraction data extracted from the NMO stack in Figure~\ref{fig:gom-best-model-nmo-stk} using plane-wave destruction filters.}

\multiplot{2}{gom-best-model-image,gom-best-model-difr-img}{width=0.65\columnwidth}{Kirchhoff time images generated using the lowest cost continuation model shown in Figure~\ref{fig:gom-best-model-final-plot} corresponding to (a) the complete NMO stack data displayed in Figure~\ref{fig:gom-best-model-nmo-stk}; (b) the diffraction data shown in Figure~\ref{fig:gom-best-model-nmo-stk-difr}.}

The continuation approach to picking velocities is able to overcome local minima and generate final models of similar cost and appearance relative to those created without continuation.  As can be seen in the cost convergence plots for the continuation approach, Figure~\ref{fig:gom-costs}, and the cost convergence plot without continuation, Figure~\ref{fig:gom-noncont-costs}, the continuation approach leads to uniformly low-cost models relative to the approach without continuation.  Indeed, the lowest-cost model achieved with continuation possesses a cost of $\tilde{G}(v)=129.597$ while the highest cost model possesses a cost of $\tilde{G}(v)=129.636$.  These costs are significantly lower than the starting model costs for the 125 constant-gradient models, which range from roughly $\tilde{G}(v_o)=135$ to $\tilde{G}(v_o)=148$.  Examining Figure~\ref{fig:gom-costs}, none of the final models from the 125 has a final cost that is visibly different from the lowest-cost model.  Comparing this result to those of cost convergence without continuation, shown in Figure~\ref{fig:gom-noncont-costs}, the lowest-cost non-continuation model has a higher cost ($\tilde{G}(v)=130.278$) than the highest-cost continuation model.  There is also a significantly larger spread in final costs.

The continuation approach tends to generate to models that appear similar.  The lowest-cost continuation model, plotted in Figure~\ref{fig:gom-best-model-final-plot}, and the highest-cost continuation model in Figure~\ref{fig:gom-worst-model-final-plot} are visibly the same.  Areas where the continuation models tend to differ can be seen in the mean continuation final model $H^1$ misfit plot of Figure~\ref{fig:gom-models-h1mis}.  This appears to be primarily confined to two approximately horizontal features near 0.5 and 1 s, which coincide with areas where velocity changes rapidly in the lowest-cost continuation final model, Figure~\ref{fig:gom-best-model-final-plot}.   The lowest-cost non-continuation model in Figure~\ref{fig:gom-nocont-best-model-final-plot} has a reasonably similar appearance to the lowest-cost continuation final model, although it possesses some anomalous ``blobs'' that are not geologically plausible. The highest-cost non-continuation final model in Figure~\ref{fig:gom-nocont-worst-model-final-plot} bears little, if any, resemblance to the lowest-cost continuation final model, and does not appear particularly geological or informative of the subsurface.  Mean $H^1$ misfit for the non-continuation final models in Figure~\ref{fig:gom-nocont-models-h1mis} tends to be significantly larger than that for final models determined using continuation.  This plot has interesting dendritic features whose cause we do not understand, but may be related to the structure of local minima. Examining the $H^1$ convergence plot of the continuation approach, all but four of the 125 initial constant-gradient velocity models achieve misfits with the lowest-cost continuation model lower than $10^{-6}$ (note that seismic processing operations as well as these calculations were performed using single precision arithmetic, so some of this misfit may be due to rounding error).  Most of these models follow paths that significantly decrease their $H^1$ misfit with the lowest-cost model as the number of iterations increase, causing them to move ``toward'' the lowest-cost model.  Conversely, the evolution of $H^1$ misfit for non-continuation models has no obvious decreasing trend. Rather, the misfit of these models actually tends to increase, indicating they are moving ``away'' from the lowest-cost model.  No non-continuation models possess misfit values less than $10^{-2}$, and the smallest misfit is actually attained by a starting model.

The semblance scan overlaid by lowest cost continuation velocity in the left panel of Figure~\ref{fig:gom-gather-0} shows a picked velocity that tracks the dominant trend in the semblance scan.  The right panel of that figure shows the corresponding NMO corrected gather.  Events in that gather are flat and laterally coherent, indicating that the picked velocity does a good job of performing NMO correction.  The NMO stack, which is generated by applying the NMO correction corresponding to the lowest cost continuation velocity to all gathers and then summing over offset, is shown in Figure~\ref{fig:gom-best-model-nmo-stk}.  Energy in the stack is both focused and laterally coherent, again indicating that the NMO correction performs as intended. The diffraction data in Figure~\ref{fig:gom-best-model-nmo-stk-difr} appear as expected -- most energy present in that data have the hyperbolic moveout associated with seismic diffraction.  Migration using the lowest cost continuation picked velocity produces a complete image in Figure~\ref{fig:gom-best-model-image} that has well-defined, laterally coherent reflections that are often interrupted by discontinuities indicative of faulting.  The diffraction features present in Figure~\ref{fig:gom-best-model-difr-img} are collapsed to points, providing further confirmation of the quality of the picked velocities.  These diffractions tend to delineate the faults which can be seen in the discontinuities of Figures~\ref{fig:gom-best-model-image} and \ref{fig:gom-best-model-difr-img}. 

The lowest-cost final velocity model output by the continuation picking method produces quality complete and diffraction seismic images when used as part of a seismic processing workflow on the Gulf of Mexico field data set featured in this section.


\subsection{Heidrun Field Horizon Picking}
\inputdir{horizon}
To illustrate the versatility of the proposed variational picking method, we apply it to an automatic horizon interpretation problem.  Seismic horizons are laterally continuous features in seismic images representing isochrons, or constant levels of geologic time \cite[]{vail-1977}.
  Identifying and mapping seismic horizons is a key step in seismic interpretation, enabling earth scientists to delineate subsurface structures, stratigraphy, and the volumetrics of potential subsurface reservoirs \cite[]{wu-2013}.  Because this activity is such an essential step in subsurface evaluation, much research has focused on developing computer algorithms to automatically accomplish the task (\citealp{lomask2006flattening,fomel2010predictive,hoyes2011review,wu2015horizon,wu2018least,xue2018predictive,peters2019multiresolution}).  The picking method presented here is not intended to replace this body of research, but rather to be seen as a tool that could work in tandem with it.  One approach for automatic horizon interpretation involves outputting a volume containing the likelihood for a horizon to exist at each point in space.  For example, \cite{shi-2020} use waveform embedding to extract seismic horizons using a deep convolutional network.  The output of this deep convolutional network is a horizon probability volume, and the horizon is determined by selecting the positions with high probability values.  The variational picking approach of this paper works well with advanced techniques which output semblance-like volumes, including probability volumes, used to determine a horizon surface.  Because smoothness is not explicitly imposed with the variational approach proposed in this paper, it has the ability to track the rugose features frequently observed in seismic data.

The semblance-like volume for horizon likelihood employed here is less advanced than some of those appearing in the literature.  In our approach, which is detailed in Appendix~\ref{horizonpick}, a trace representative of a seismic horizon is selected, cosine tapering is applied to its edges, and it is padded with zeros.  To measure how well that ideal waveform matches with other traces throughout the seismic image volume, the crosscorrelation is calculated throughout the volume over shift $\tau$. The minimum crosscorrelation value is subtracted so the output semblance-like volume is non-negative.  Automatic picking may be performed on that volume to determine the shifts defining the horizon.  Those shifts may be converted back to time in the image domain by adding a reference time for the reference horizon trace, taken here to be the trace's midpoint.


\multiplot{2}{hcube,hcube-w}{width=0.65\columnwidth}{(a) Seismic image from the Heidrun Field; (b) zoomed portion of that image centered on the horizon that will be automatically picked.}

Figure~\ref{fig:hcube} contains a seismic image from the Heidrun Field off the coast of Norway, and Figure~\ref{fig:hcube-w} contains a zoomed portion of that image.  We will focus on automatically picking the horizon defined by the ``double white peak'' in Figure~\ref{fig:hcube-w}.  A waveform representing an ideal version of that horizon is selected, and that horizon reference trace is plotted in Figure~\ref{fig:heidrun-reference}.

\plot{heidrun-reference}{width=0.7\columnwidth}{Reference trace for horizon that will be picked from Figure~\ref{fig:hcube-w}.}
\plot{heidrun-alpha-scalebar}{width=0.7\columnwidth}{Cross correlation between reference trace and seismic image.}

Crosscorrelation values between the horizon reference trace and the seismic image are computed over a variety of shifts according to Equations~\ref{eq:horiz-xcor} and \ref{eq:alpha-horiz}.  The shift coordinate $\tau$ is transformed to image time $t$ to be representative of horizon location and displayed in Figure~\ref{fig:heidrun-alpha-scalebar}.  Continuation picking is applied to the crosscorrelation volume using a flat starting model of 1.9745 s using 12 continuation levels.  The evolution of the picked horizon every ten iterations, beginning with the starting model, is shown in Figures~\ref{fig:heidrun-starting-updates-itr-0-contour} through \ref{fig:heidrun-starting-updates-itr-8-contour}.  The final picked horizon is shown with constant horizon time contours in Figure~\ref{fig:heidrun-horiz-out-11-contour}. The cost convergence is calculated for each model update using the semblance-like volume in Figure~\ref{fig:heidrun-alpha-scalebar} and displayed in Figure~\ref{fig:heidrun-starting-updates-costs}.

\multiplot{9}{heidrun-starting-updates-itr-0-contour,heidrun-starting-updates-itr-1-contour,heidrun-starting-updates-itr-2-contour,heidrun-starting-updates-itr-3-contour,heidrun-starting-updates-itr-4-contour,heidrun-starting-updates-itr-5-contour,heidrun-starting-updates-itr-6-contour,heidrun-starting-updates-itr-7-contour,heidrun-starting-updates-itr-8-contour}{width=0.29\columnwidth}{Evolution of continuation picking horizon overlaid by constant horizon time contours.}

\plot{heidrun-horiz-out-11-contour}{width=0.65\columnwidth}{Final picked horizon with constant horizon time contours.}

\plot{heidrun-starting-updates-costs}{width=0.7\columnwidth}{Cost convergence for continuation horizon picking using the semblance-like volume shown in Figure~\ref{fig:heidrun-alpha-scalebar}.}

To illustrate the picked horizon in the seismic volume, we generate a series of overlay images.  Note that the position of the horizon is taken to be at the halfway point of the reference trace in Figure~\ref{fig:heidrun-reference}, and thus we would expect it to approximately  track the trough between the double white peaks in the image.  Constant-inline slices are shown for inline 100 in Figure~\ref{fig:hcube-wind-il-overlay-0} and  inline 300 in Figure~\ref{fig:hcube-wind-il-overlay-2}. Constant-crossline slices are similarly generated and shown for crossline 1200 in Figure~\ref{fig:hcube-wind-xl-overlay-1} and crossline 1400 in Figure~\ref{fig:hcube-wind-xl-overlay-3}.


\multiplot{4}{hcube-wind-il-overlay-0,hcube-wind-il-overlay-2,hcube-wind-xl-overlay-1,hcube-wind-xl-overlay-3}{width=0.45\columnwidth}{Picked horizon from Figure~\ref{fig:heidrun-horiz-out-11-contour} overlaid on Heidrun seismic image from Figure~\ref{fig:hcube-w} for: (a) Inline 100; (b) Inline 300; (c) Crossline 1200; (d) Crossline 1400.}

The variational picking method outlined here appears to successfully pick the desired horizon from a poorly informed, flat starting model.  Examining the picked horizon image overlays (Figures~\ref{fig:hcube-wind-il-overlay-0} through \ref{fig:hcube-wind-xl-overlay-3}) the horizon successfully tracks the trough between the two bright, white peaks, even when that position changes relatively rapidly, as is the case near crossline 1050 in Figure~\ref{fig:hcube-wind-il-overlay-0}, or when the character of the horizon wavelet changes somewhat, as is the case near inline 560 in Figure~\ref{fig:hcube-wind-xl-overlay-3}.  The display of the picked horizon in Figure\old{s}~\ref{fig:heidrun-horiz-out-11-contour} shows how the method is able to identify a plunging anticline structure in the horizon. The horizon-quality-of-fit volume defined by Equation~\ref{eq:alpha-horiz} is primitive, and may not be well-suited for the automatic interpretation of other horizons which are faulted or where there are significant changes in the wavelet.  However, when applied to the horizon in this study, and used in conjunction with the continuation-variational picking method, it is able to produce a representation of the horizon from the single seed wavelet shown in Figure~\ref{fig:heidrun-reference}.  When used with more sophisticated measures of horizon location probability, the variational approach outlined here could help produce high-quality automatic interpretations of subsurface features with minimal human intervention.

\section{Conclusions}
We propose a variational method for picking velocity surfaces from semblance-like volumes.  When coupled with a continuation approach, this method is able to avoid many local minima. Using a discretized version of the variational statement with a $\ell$-BFGS algorithm enables the method to converge more rapidly.

The variational method outlined here for determining optimal surfaces from semblance-like volumes is more computationally expensive than existing methods for determining optimal lines from semblance panels.  However, the ability of the method to incorporate spatially adjacent information without explicitly imposing smoothing on the model justifies the added expense.  Direct application of the method suffers from the multimodality and non-convexity of the proposed objective function and, unless a particularly well-informed starting model is used, the method is likely to converge to a local minimum.  Such local minima may differ significantly from the global minimum.  This difficulty may be overcome by applying the variational picking scheme with a continuation approach.
Although continuation increases the cost of the method through the creation of additional smoothed semblance volumes and the application of the picking scheme to each smoothed volume, the ability of the method to avoid local minima and find a superior final model justifies the expense. 

Applying the proposed approach to field data sets from the Viking Graben and the Gulf of Mexico shows how it is able to determine geologically plausible velocity models.  The use of continuation both enables the scheme to behave more like a global minimizer, and allows models output by the approach to vary substantially from the starting model.  Using the lowest-cost output model for each data set in seismic processing workflows produces quality images and further shows how the method may be incorporated as a tool for seismic processors.

The versatility of the variational method is demonstrated by deploying it to automatically pick a seismic horizon from the Heidrun Field. Because smoothing is not explicitly imposed on the model, and is only used during the continuation process, the approach is able to determine a seismic horizon that changes rapidly in space to follow the reflector.

This method solves for minimizing surfaces in $H^1$, a space of smoothly varying functions.  Minimizing surfaces are guaranteed to exist in this space because of the presence of a strictly positive $\epsilon$ in Equation~\ref{eq:stableminfunc}, which is essential for demonstrating the existence of minimizers and proving that iterative schemes converge to those minimizers in an infinite-dimensional setting.  Because RMS velocities are, in a sense, the integral of interval velocities, which exist in $L^2$, or equivalently are square integrable functions, all RMS velocities will exist in $H^1$. Therefore, this is not a limitation.  However, if one is interested in solving for best-fit surfaces which are not smoothly varying but may feature discontinuities, they would want to set $\lambda$ to a high value and $\epsilon$ to $0$.  Although having $\epsilon = 0$ violates the assumptions used in proving the existence of and convergence to minimizers of the variational method, in practice, minimizers are found to exist and can be iteratively determined.

The variational statement can be thought of as a modified soap bubble problem.  It determines minimal area surfaces using a weight which rewards tracking highs in the semblance-like volume. If intervals within the semblance-like volume without high values exist, the method should output a minimal-area, or constant-gradient, surface connecting the high values bounding the region.  This situation corresponds to intervals in reflection velocity scans without reflection energy. The method assumes that highs in the semblance-like volume correctly track the value of the parameter.  Thus, the largest source of manual intervention in the examples shown in this paper comes from defining the functions to mute artificially high values from the semblance-like volumes where this assumption does not hold.  Developing methods to automatically generate those mutes is a promising direction for future study.

Beyond the demonstrated applications shown here for processing 2D seismic lines and automatically interpreting seismic horizons, numerous promising directions for future study involving the application of this variational approach exist. Examples include using the method to generate starting models for full waveform inversion, extending the software to picking 3D velocity volumes from 4D semblance hypervolumes, calculating time-shifts to match time-lapse seismic volumes, or applying the method to other situations where one wishes to determine a laterally continuous surface.  These extensions could be accomplished using the same variational statement featured in this paper.  Modifying that variational statement could enable the creation of a method for simultaneously determining surfaces for multiple parameters, which would be useful for generating anisotropic models.

\section{Acknowledgements}
We are grateful to TCCS sponsors for financial support of this work; to Todd Arbogast, Doug Foster, Omar Ghattas, Tom Hess, Tristan Leeuwen, Nick Luiken, and Mary Wheeler for inspiring conversations; the reviewers of this manuscript for valuable suggestions; and and to contributors to the \texttt{Madagascar} open-source software library \cite[]{madagascar}.

\appendix
\section{Appendix A: Pseudocode for a Picking Algorithm Utilizing Continuation}
Algorithm~\ref{alg:picking} contains pseudocode which could be used to create a variational picking algorithm.  Algorithm~\ref{alg:continu} shows how a variational picking algorithm may be used in a continuation framework.  Here, $\rho_o$ and $\rho_f$ are strictly positive numbers denoting the minimum and maximum possible step sizes at each iteration, $\gamma$ and $\xi$ are positive early termination parameters, $N$ is the maximum number of iterations at each continuation level, $v(\mathbf{x})$ is a velocity model, $v_0(\mathbf{x})$ is a starting velocity model, $g(\mathbf{x})$ is the functional gradient, and $h(\mathbf{x})$ is the search direction. $M$ is the number of continuation levels,  $\mathbf{r}_j$ is a vector holding the triangle smoothing radius for each spatial dimension at continuation level $j$, and $\sigma_j$ is the semblance scaling factor at continuation level $j$. We assume that scaling and smoothing increase with increasing $j$. $\alpha_0 \left[v,\mathbf{x}\right]$ is the least-smoothed semblance-like volume, and $\alpha \left[v,\mathbf{x}\right]$ denotes a semblance-like volume.  These algorithms assume one has the following methods defined:
\begin{itemize}
	\item \textsc{Cost}$\left(v(\mathbf{x}),\alpha \left[v,\mathbf{x}\right],\lambda,\epsilon \right)$ \\
	Returns the cost, $\tilde{G}[v]$ according to Equation~\ref{eq:stableminfunc}, associated  with model $v(\mathbf{x})$ for semblance-like volume $\alpha \left[v,\mathbf{x}\right]$ using regularization parameters $\lambda$ and $\epsilon$.  The returned object is a scalar.
	\item \textsc{Gradient}$\left( v(\mathbf{x}),\alpha\left[v,\mathbf{x} \right], \lambda, \epsilon \right)$ \\ Returns the functional gradient, $\nabla \tilde{G}[v]$ according to Equation~\ref{eq:funcgradgfull}, associated with model $v(\mathbf{x})$ for semblance-like volume $\alpha \left[v,\mathbf{x} \right]$ using regularization parameters $\lambda$ and $\epsilon$.  The returned object is a function with the same domain as $v(\mathbf{x})$.
	\item \textsc{UpdateSearchDirection}$\left( g(\mathbf{x}), \mathbf{s} \right)$ \\ Returns an updated search direction using gradient $g(\mathbf{x})$ according to a quasi-Newton scheme, for example $\ell$-BFGS. In this method, $\mathbf{s}$ is an abstract state vector for the quasi-Newton scheme.  The object $\mathbf{s}$ contains hyperparameters related to the method as well as gradient or search direction information from previous iterations which is used in the construction of the updated search direction.  Application of the method is assumed to update abstract state $\mathbf{s}$ without returning it as an output.  The returned object is a function with the same domain as $g(x)$.
	\item \textsc{LineSearch}$\left(h(\mathbf{x}),v(\mathbf{x}),\alpha \left[v,\mathbf{x}\right], \rho_{o}, \rho_{f}, \lambda, \gamma \right)$ \\ Returns the step size $\rho$ between minimum and maximum step sizes $\rho_{o}$ and $\rho_{f}$ using a method of one's choosing such that
	\begin{equation}
		\label{eq:lineasearch}
		\rho = \underset{\rho \in \left[\rho_{o}, \rho_{f} \right]}{\arg \min} \textsc{Cost}\left( v(\mathbf{x})+\rho h(\mathbf{x}), \alpha \left[v,\mathbf{x}\right], \lambda, \epsilon \right).
	\end{equation}
	The returned object is a scalar.
	\item \textsc{Smooth}$\left( \alpha \left[v,\mathbf{x}\right], \mathbf{r} \right)$ \\ Applies triangle smoothing to $\alpha \left[v,\mathbf{x}\right]$ using smoothing radii for each dimension of $\alpha$ contained in $\mathbf{r}$.  The returned object is a smoothed function with the same domain as $\alpha \left[v,\mathbf{x}\right]$
\end{itemize}

\begin{algorithm}[H]
\caption{Variational Picking}\label{alg:picking}
\begin{algorithmic}[1]
	\Function{VariationalPick}{$v_0(\mathbf{x}), \alpha\left[v,\mathbf{x}\right], N, \rho_{o}, \rho_{f}, \lambda, \epsilon$}
	\State Initialize arrays $v(\mathbf{x})$, $g(\mathbf{x})$, $z(\mathbf{x})$
	\State Initialize search direction state vector $\mathbf{s}$
	\State $v(\mathbf{x}) = v_0(\mathbf{x})$ \Comment{initialize model with starting model}
	\State c = \textsc{Cost}($v(\mathbf{x}),\alpha \left[v,\mathbf{x}\right] , \lambda, \epsilon)$) \Comment{compute starting model cost}
	\For{$i=1:N$} \Comment{Iterate through maximum number of iterations}
		\State $g(\mathbf{x})$ = \textsc{Gradient}($v(\mathbf{x}),\alpha\left[v,\mathbf{x}\right],\lambda, \epsilon$) \Comment{compute gradient}
		\State $h(\mathbf{x})$ = \textsc{UpdateSearch}($g(\mathbf{x}), \mathbf{s}$) \Comment{update search direction and state vector $\mathbf{s}$}
		\State $\rho = $\textsc{LineSearch}($h(\mathbf{x}),v(\mathbf{x}),\alpha \left[v,\mathbf{x}\right], \rho_{o}, \rho_{f}, \lambda, \epsilon$) \Comment{determine step size}
		\State $\tilde{c} = $\textsc{Cost}($v(\mathbf{x})+\rho h(\mathbf{x}),\alpha \left[v,\mathbf{x}\right], \lambda, \epsilon )$) \Comment{Cost of updated model}
		\If{$\tilde{c} > c$}
			\State \textbf{break} \Comment{increasing cost implies divergence, return model without update}
		\EndIf
		\State $v(\mathbf{x})$ = $v(\mathbf{x}) + \rho h(\mathbf{x})$ \Comment{update model}
		\If{$\left| \left| \rho h(\mathbf{x}) \right| \right| < \gamma $ or $c - \tilde{c} < \xi$}
			\State \textbf{break} \Comment{sufficiently small model update or cost decrease implies convergence}
		\EndIf
		\State $c = \tilde{c}$ \Comment{update cost}
	\EndFor
	\State \textbf{return} $v(\mathbf{x})$ \Comment{return final model}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Continuation Picking}\label{alg:continu}
	\begin{algorithmic}[1]
	\Function{ContinuationPick}{$v_0(\mathbf{x}), \alpha_0\left[v,\mathbf{x}\right],M, N, \{ \sigma_j \}_{j=1}^M, \{ \mathbf{r}_j \}_{j=1}^M, \rho_{o}, \rho_{f}, \lambda, \epsilon$}
		\State $v(\mathbf{x}) = v_0(\mathbf{x})$ \Comment{initialize model with starting model}
		\For{$j=M:1$} \Comment{iterate from highest continuation level to lowest}
			\State $\alpha \left[v,\mathbf{x}\right] = \sigma_j \, \textsc{Smooth}\left( \alpha_0 \left[v,\mathbf{x}\right], \mathbf{r}_j \right)$ \Comment{smooth and scale semblance}
			\State $v(\mathbf{x}) = \textsc{VariationalPick}\left(v(\mathbf{x}), \alpha\left[v,\mathbf{x}\right], N, \rho_{o}, \rho_{f}, \lambda, \epsilon \right)$ \Comment{perform variational picking, update model}
		\EndFor
		\State \textbf{return} $v(\mathbf{x})$ \Comment{return final model}
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\appendix
\section{Appendix B: A Primitive, Semblance-Like Volume for Horizon Picking}
Suppose we have a trace representative of a seismic horizon, apply cosine tapering to its edges, and pad it  with zeros.  This ideal horizon reference trace is called $h(t)$.  To create a measure of how well this ideal waveform matches with other traces throughout a seismic volume, the crosscorrelation is calculated throughout the volume over shift $\tau$,
\begin{equation}
\label{eq:horiz-xcor}
\gamma(\tau,\mathbf{x}) = \int h(t+\tau)d(t,\mathbf{x}) d\tau,
\end{equation}
where $d(t,\mathbf{x})$ is the seismic image.  In order to ensure that the volume we generate is non-negative, let
\begin{equation}
\label{eq:alpha-horiz}
\alpha(\tau,\mathbf{x}) = \gamma(\tau,\mathbf{x}) - \min_{\left(\tau,\mathbf{x}\right)} \left[ \gamma(\tau,\mathbf{x})\right] .
\end{equation}
Automatic picking may be performed on the semblance-like volume $\alpha(\tau,\mathbf{x})$ to determine the shifts defining the horizon corresponding to reference trace $h(t)$.  Those shifts may be converted back to time in the image domain by adding a reference time for the ideal horizon trace, taken here to be that trace's midpoint.

This is a basic method for creating a horizon probability volume, which is intended for use in this paper as part of a proof of concept for automatic horizon interpretation using the variational picking algorithm.  It could encounter difficulties in areas where significant lateral wavelet variation or faulting occurs.


\bibliographystyle{seg}
\bibliography{ref}
